{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import clear_output  # this has to be removed if used on a single script\n",
    "import random\n",
    "from preprocess import preprocess_seismo, preprocess_coord\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import GPy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein Generative Adversarial Networks - Gradient Penalty (WGAN-GP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(x, y, mb_size):\n",
    "    idx = np.arange(len(x), dtype = np.int64)\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:mb_size]\n",
    "    return x[idx], y[idx], idx\n",
    "\n",
    "def plot(x, index=0):\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.plot(x, label='Reconstructed')\n",
    "    plt.plot(X_data[index], label='Real')\n",
    "    plt.legend()\n",
    "\n",
    "def plot_test(x, index=0):\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.plot(x, label='Reconstructed')\n",
    "    plt.plot(X_data_test[index], label='Real')\n",
    "    plt.legend()\n",
    "\n",
    "def calculate_R2(original, prediction, label, store):\n",
    "    AM = original.mean()\n",
    "    BM = prediction.mean()\n",
    "    c_vect = (original-AM)*(prediction-BM)\n",
    "    d_vect = (original-AM)**2\n",
    "    e_vect = (prediction-BM)**2\n",
    "    r_out = np.sum(c_vect)/float(np.sqrt(np.sum(d_vect)*np.sum(e_vect)))\n",
    "    print(label+str(r_out))\n",
    "    store.append(r_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define number of time components in seismograms, number of coordinates, train/test split and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dim = 501 # size of the seismograms\n",
    "y_dim = 4\n",
    "# load data\n",
    "split = 2000\n",
    "test_valid = 1000\n",
    "X_data_ = np.load('./seismograms_4000seismo_ISO.npy')[:, :X_dim]\n",
    "y_data_ = np.load('./coordinates_4000seismo_ISO.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess coordinates\n",
    "y_data_preprocessed, meancoords, stdcoords = preprocess_coord(y_data_, split=split, test_valid=test_valid, sort=False, std=True)\n",
    "y_data = y_data_preprocessed[:split]\n",
    "y_data_valid = y_data_preprocessed[split:split+test_valid]\n",
    "y_data_test = y_data_preprocessed[split+test_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess seismograms\n",
    "X_data_preprocessed = preprocess_seismo(X_data_, split, log=False, std=True, rescale=True, rescale_onlyamp=False)\n",
    "X_data = X_data_preprocessed[:split]\n",
    "X_data_valid =  X_data_preprocessed[split:split+test_valid]\n",
    "X_data_test =  X_data_preprocessed[split+test_valid:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the architecture of the generator and of the critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb_size = 256 # batch size\n",
    "z_dim = 64 # dimension of latent space\n",
    "h1_dim = 256 # dimension of the hidden layer, i.e. number of nodes\n",
    "h2_dim = 128 # dimension of the hidden layer, i.e. number of nodes\n",
    "h3_dim = 64 # dimension of the hidden layer, i.e. number of nodes\n",
    "h4_dim = 32 # dimension of the hidden layer, i.e. number of nodes\n",
    "h_dim = 50 # final part of the discriminator\n",
    "act_f = 'leaky_relu' # activation function\n",
    "lr = 1e-4  # learning rate\n",
    "lambda_ = 10.0\n",
    "num_critic = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "c = tf.placeholder(tf.float32, shape=[None, y_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "\n",
    "def generator(coord, zed):\n",
    "    with tf.variable_scope('g_'):\n",
    "        inputs = tf.concat(axis=1, values=[zed, coord])\n",
    "        h4 = tf.layers.dense(coord, h4_dim, activation=getattr(tf.nn, act_f))\n",
    "        h3 = tf.layers.dense(h4, h3_dim, activation=getattr(tf.nn, act_f))\n",
    "        h2 = tf.layers.dense(h3, h2_dim, activation=getattr(tf.nn, act_f))\n",
    "        h = tf.layers.dense(h2, h1_dim, activation=getattr(tf.nn, act_f))\n",
    "        logits = tf.layers.dense(h, X_dim)\n",
    "        return logits\n",
    "\n",
    "def discriminator(data, coord, is_reuse=False):\n",
    "    with tf.variable_scope('d_') as scope:\n",
    "        if is_reuse is True:\n",
    "            scope.reuse_variables()\n",
    "        h2 = tf.layers.dense(data, h1_dim, activation=getattr(tf.nn, act_f))\n",
    "        h2_cat = tf.concat(axis=1, values=[h2, coord])\n",
    "        h = tf.layers.dense(h2_cat, h_dim, activation=getattr(tf.nn, act_f))\n",
    "        logits = tf.layers.dense(h, 1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_samples = generator(c, z)\n",
    "d_logit_real = discriminator(X, c)\n",
    "d_logit_fake = discriminator(g_samples, c, is_reuse=True)\n",
    "\n",
    "# discriminator loss\n",
    "wgan_d_loss = tf.reduce_mean(d_logit_fake) - tf.reduce_mean(d_logit_real)\n",
    "\n",
    "# generator loss\n",
    "g_loss = -tf.reduce_mean(d_logit_fake)\n",
    "\n",
    "d_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='d_')\n",
    "g_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='g_')\n",
    "\n",
    "# gradient penalty\n",
    "alpha = tf.random_uniform(shape=[tf.shape(X)[0], 1], minval=0., maxval=1.)\n",
    "differences = g_samples - X\n",
    "interpolates = X + (alpha * differences)\n",
    "gradients = tf.gradients(discriminator(interpolates, c, is_reuse=True), [interpolates])[0]\n",
    "slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1]))\n",
    "gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "\n",
    "d_loss = wgan_d_loss + lambda_ * gradient_penalty\n",
    "\n",
    "# Optimizers for generator and discriminator\n",
    "gen_optim = tf.train.AdamOptimizer(\n",
    "    learning_rate=lr, beta1=0.0, beta2=0.9).minimize(g_loss, var_list=g_vars)\n",
    "dis_optim = tf.train.AdamOptimizer(\n",
    "    learning_rate=lr, beta1=0.0, beta2=0.9).minimize(d_loss, var_list=d_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_wgan, losses_gp, losses_d, losses_g = [], [], [], []\n",
    "losses_wgan_valid, losses_gp_valid, losses_d_valid, losses_g_valid = [], [], [], []\n",
    "\n",
    "training_WGANGP_R2 = []\n",
    "validation_WGANGP_R2 = []\n",
    "test_WGANGP_R2 = []\n",
    "\n",
    "n_iter = 1000000 # number of iterations\n",
    "best_loss = 1e8\n",
    "stopping_step = 0\n",
    "patience = 200\n",
    "\n",
    "for it in range(n_iter):\n",
    "    \n",
    "    # train critic\n",
    "    for idx in range(num_critic):\n",
    "        random_perm = np.random.permutation(X_data.shape[0])\n",
    "        indices = random_perm[:mb_size]\n",
    "        _, wgan_d_loss_, gp_loss_, d_loss_ = sess.run([dis_optim, wgan_d_loss, gradient_penalty, d_loss], feed_dict={X: X_data[indices], c: y_data[indices], z: np.random.uniform(-1., 1., size=[mb_size, z_dim])})\n",
    "\n",
    "    # train generator\n",
    "    random_perm = np.random.permutation(X_data.shape[0])\n",
    "    indices = random_perm[:mb_size]     \n",
    "    _, g_loss_ = sess.run([gen_optim, g_loss], feed_dict={X: X_data[indices], c: y_data[indices], z: np.random.uniform(-1., 1., size=[mb_size, z_dim])})\n",
    "\n",
    "    # metrics\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # test set performance, only wgan loss\n",
    "    dis_feed = {c: y_data_valid, X: X_data_valid, z: np.random.uniform(-1., 1., size=[X_data_valid.shape[0], z_dim])}\n",
    "    dis_run = [wgan_d_loss, gradient_penalty, d_loss]\n",
    "    wgan_d_loss_valid, gp_loss_valid, d_loss_valid = sess.run(dis_run, feed_dict=dis_feed)\n",
    "    \n",
    "    # generator validation loss\n",
    "    gen_feed = {c: y_data_valid, X: X_data_valid, z: np.random.uniform(-1., 1., size=[X_data_valid.shape[0], z_dim])}\n",
    "    _, g_loss_valid = sess.run([gen_optim, g_loss], feed_dict=gen_feed)\n",
    "    \n",
    "    # negative critic loss\n",
    "    #losses_wgan.append(-wgan_d_loss_)\n",
    "    #losses_gp.append(gp_loss_)\n",
    "    losses_d.append(-d_loss_)\n",
    "    #losses_g.append(g_loss_)\n",
    "\n",
    "    losses_d_valid.append(-d_loss_valid)\n",
    "    #losses_gp_valid.append(gp_loss_valid)\n",
    "    #losses_g_valid.append(g_loss_valid)\n",
    "\n",
    "    print('Iter: {}'.format(it))\n",
    "    print('Discriminator loss: {:.4}'.format(np.mean(-d_loss_)))\n",
    "    print('Validation discriminator loss: {:.4}'.format(np.mean(-d_loss_valid)))\n",
    "\n",
    "    #X_R2, y_R2, _ = next_batch(X_data, y_data, test_valid)\n",
    "    #prediction_training = sess.run(g_samples, feed_dict={c: y_R2, z: np.random.uniform(-1., 1., size=[test_valid, z_dim])})\n",
    "    #calculate_R2(X_R2, prediction_training, 'Training WGANGP: ', training_WGANGP_R2)\n",
    "    #prediction_validation = sess.run(g_samples, feed_dict={c: y_data_valid, z: np.random.uniform(-1., 1., size=[y_data_valid.shape[0], z_dim])})\n",
    "    #calculate_R2(X_data_valid, prediction_validation, 'Validation WGANGP: ', validation_WGANGP_R2)\n",
    "    #prediction_test = sess.run(g_samples, feed_dict={c: y_data_test, z: np.random.uniform(-1., 1., size=[y_data_test.shape[0], z_dim])})\n",
    "    #calculate_R2(X_data_test, prediction_test, 'Test WGANGP: ', test_WGANGP_R2)\n",
    "\n",
    "    loss_value = np.mean(-d_loss_valid)\n",
    "    if it > 100:\n",
    "        if loss_value < best_loss:\n",
    "            stopping_step = 0\n",
    "            best_loss = loss_value\n",
    "            save_path = saver.save(sess, f\"./saved_models_iso_WGANGP/best_model.ckpt\")\n",
    "        else:\n",
    "            stopping_step += 1\n",
    "            print(f'Early stopping: {stopping_step}/{patience}')\n",
    "        if stopping_step >= patience:\n",
    "            print(f'Patience limit reached at iteration {it}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(10, 4))\n",
    "# plt.plot(losses_d, label='Training')        \n",
    "# plt.plot(losses_d_valid, label='Validation')        \n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "#print(f'Best model was model {len(losses_d) - patience}')\n",
    "load_path = (\"./saved_models_iso_WGANGP/best_model.ckpt\")\n",
    "saver.restore(sess, load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP training for Amplitude and Time shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_rescale = np.loadtxt('./amplitude_rescale_NOTsorted.txt')\n",
    "shift_index = np.loadtxt('./shift_index_NOTsorted.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_rescale_train = amplitude_rescale[:split].reshape(X_data.shape[0], 1)\n",
    "amplitude_rescale_valid = amplitude_rescale[split:split+test_valid]\n",
    "amplitude_rescale_test = amplitude_rescale[split+test_valid:]\n",
    "\n",
    "shift_index_train = shift_index[:split].reshape(X_data.shape[0], 1)\n",
    "shift_index_valid = shift_index[split:split+test_valid]\n",
    "shift_index_test = shift_index[split+test_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit GP to data rescaling\n",
    "# amplitude\n",
    "kern = GPy.kern.Matern32(4,ARD=True)\n",
    "n = GPy.models.GPRegression(y_data, amplitude_rescale_train, kern)\n",
    "n.optimize(messages=True, max_f_eval = 1000)\n",
    "\n",
    "y_pred_train = n.predict(y_data)[0]\n",
    "y_pred_train = y_pred_train[:, 0]\n",
    "calculate_R2(amplitude_rescale_train.flatten(), y_pred_train, 'Amplitude R2 train: ', [])\n",
    "\n",
    "y_pred_valid = n.predict(y_data_valid)[0]\n",
    "y_pred_valid = y_pred_valid[:, 0]\n",
    "calculate_R2(amplitude_rescale_valid, y_pred_valid, 'Amplitude R2 validation: ', [])\n",
    "\n",
    "# this is to be used later\n",
    "y_pred_test = n.predict(y_data_test)[0]\n",
    "y_pred_test = y_pred_test[:, 0]\n",
    "\n",
    "#plt.plot(amplitude_rescale_test, color='blue')\n",
    "#plt.show()\n",
    "#plt.plot(y_pred_test, color='red')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time shift\n",
    "ker = GPy.kern.Matern32(4,ARD=True)\n",
    "m = GPy.models.GPRegression(y_data,shift_index_train,ker)\n",
    "m.optimize(messages=True,max_f_eval = 1000)\n",
    "\n",
    "y_pred_train_2 = m.predict(y_data)[0]\n",
    "y_pred_train_2 = y_pred_train_2[:, 0]\n",
    "calculate_R2(shift_index_train.flatten(), y_pred_train_2, 'Time shift R2 training: ', [])\n",
    "\n",
    "y_pred_valid_2 = m.predict(y_data_valid)[0]\n",
    "y_pred_valid_2 = y_pred_valid_2[:, 0]\n",
    "calculate_R2(shift_index_valid, y_pred_valid_2, 'Time shift R2 validation: ', [])\n",
    "\n",
    "y_pred_test_2 = m.predict(y_data_test)[0]\n",
    "y_pred_test_2 = y_pred_test_2[:, 0]\n",
    "\n",
    "#plt.plot(shift_index_test, color='blue')\n",
    "#plt.show()\n",
    "#plt.plot(y_pred_test_2, color='red')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ker = GPy.kern.Matern32(4,ARD=True) \n",
    "m_load = GPy.models.GPRegression(y_data, amplitude_rescale_train, ker, initialize=False)\n",
    "m_load.update_model(False) # do not call the underlying expensive algebra on load\n",
    "m_load.initialize_parameter() # Initialize the parameters (connect the parameters up)\n",
    "m_load[:] = np.load('./saved_models_iso_WGANGP/GPmodel_amplitude.npy') # Load the parameters\n",
    "m_load.update_model(True) # Call the algebra only once\n",
    "\n",
    "# this is to be used later\n",
    "y_pred_test = m_load.predict(y_data_test)[0]\n",
    "y_pred_test = y_pred_test[:, 0]\n",
    "\n",
    "\n",
    "\n",
    "kern = GPy.kern.Matern32(4,ARD=True) \n",
    "n_load = GPy.models.GPRegression(y_data, shift_index_train, kern, initialize=False)\n",
    "n_load.update_model(False) # do not call the underlying expensive algebra on load\n",
    "n_load.initialize_parameter() # Initialize the parameters (connect the parameters up)\n",
    "n_load[:] = np.load('./saved_models_iso_WGANGP/GPmodel_time.npy') # Load the parameters\n",
    "n_load.update_model(True) # Call the algebra only once\n",
    "\n",
    "y_pred_test_2 = n_load.predict(y_data_test)[0]\n",
    "y_pred_test_2 = y_pred_test_2[:, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_inference = time.time()\n",
    "\n",
    "coordinate = np.array([31,25,158])\n",
    "\n",
    "shifted = coordinate - np.array([41,41,244])\n",
    "distances = np.linalg.norm(shifted)\n",
    "new_coords = np.zeros(4)\n",
    "new_coords[:3] = shifted\n",
    "new_coords[-1] = distances\n",
    "new_coords = (new_coords - meancoords)/stdcoords\n",
    "new_coords = new_coords.reshape((1,y_dim))\n",
    "\n",
    "prediction_testing = sess.run(g_samples, feed_dict={c: y_data_test, z: np.random.uniform(-1., 1., size=[test_valid, z_dim])})\n",
    "mean, std = 1.0242753844768389, 514.82869074668\n",
    "prediction_testing = prediction_testing*std + mean\n",
    "y_pred_test = m_load.predict(new_coords)[0]\n",
    "y_pred_test = y_pred_test[:, 0]\n",
    "y_pred_test_2 = n_load.predict(new_coords)[0]\n",
    "y_pred_test_2 = y_pred_test_2[:, 0]\n",
    "prediction_testing = np.multiply(prediction_testing, np.repeat(1/y_pred_test, X_dim).reshape(-1, X_dim))\n",
    "for i in range(test_valid):\n",
    "    prediction_testing[i] = shift(prediction_testing[i], -y_pred_test_2[i], cval=0.)\n",
    "\n",
    "for j in range(10):\n",
    "    plt.plot(X_data_[split+test_valid+j,:], color='blue')\n",
    "    plt.plot(prediction_testing[i], color='red')\n",
    "    plt.show()\n",
    "timeinf = time.time() - start_time_inference\n",
    "print(\"timeinf\", timeinf)\n",
    "np.save(\"WGANGP_inftime.npy\", timeinf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
