{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import clear_output  # this has to be removed if used on a single script\n",
    "import random\n",
    "from preprocess import preprocess_seismo, preprocess_coord\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import GPy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(x, y, mb_size):\n",
    "    idx = np.arange(len(x), dtype = np.int64)\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:mb_size]\n",
    "    return x[idx], y[idx], idx\n",
    "\n",
    "def plot(x, index=0):\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.plot(x, label='Reconstructed')\n",
    "    plt.plot(X_data[index], label='Real')\n",
    "    plt.legend()\n",
    "\n",
    "def plot_test(x, index=0):\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.plot(x, label='Reconstructed')\n",
    "    plt.plot(X_data_test[index], label='Real')\n",
    "    plt.legend()\n",
    "\n",
    "def calculate_R2(original, prediction, label, store):\n",
    "    AM = original.mean()\n",
    "    BM = prediction.mean()\n",
    "    c_vect = (original-AM)*(prediction-BM)\n",
    "    d_vect = (original-AM)**2\n",
    "    e_vect = (prediction-BM)**2\n",
    "    r_out = np.sum(c_vect)/float(np.sqrt(np.sum(d_vect)*np.sum(e_vect)))\n",
    "    print(label+str(r_out))\n",
    "    store.append(r_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dim = 501 # size of the seismograms\n",
    "y_dim = 4\n",
    "# load data\n",
    "split = 2000\n",
    "test_valid = 1000\n",
    "X_data_ = np.loadtxt('./seismograms_4000seismo_ISO.txt')[:, :X_dim]\n",
    "y_data_ = np.loadtxt('./coordinates_4000seismo_ISO.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# analysis starts here, so we start counting time from here\n",
    "#start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shifted and standardized; not sorted\n",
      "Mean: [-1.49500000e-01  6.00000000e-02 -1.23196500e+02  1.29906166e+02], std dev: [23.03473789 23.21808347 68.8275736  64.09918073]\n"
     ]
    }
   ],
   "source": [
    "# preprocess coordinates\n",
    "y_data_preprocessed, meancoords, stdcoords = preprocess_coord(y_data_, split=split, test_valid=test_valid, sort=False, std=True)\n",
    "y_data = y_data_preprocessed[:split]\n",
    "y_data_valid =  y_data_preprocessed[split:split+test_valid]\n",
    "y_data_test =  y_data_preprocessed[split+test_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(y_dim):\n",
    "    plt.plot(y_data[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Amplitude of first peak in reference seismo', 4476.40283203125)\n",
      "('Time index of first peak in reference seismo', 204)\n",
      "Saving amplitude ratios and shift indices\n",
      "Rescaling only.\n"
     ]
    }
   ],
   "source": [
    "# preprocess seismograms\n",
    "X_data_preprocessed = preprocess_seismo(X_data_, split, log=False, std=False, rescale=True, rescale_onlyamp=False)\n",
    "X_data = X_data_preprocessed[:split]\n",
    "X_data_valid =  X_data_preprocessed[split:split+test_valid]\n",
    "X_data_test =  X_data_preprocessed[split+test_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amplitude_rescale = np.loadtxt('./amplitude_rescale_NOTsorted.txt')\n",
    "shift_index = np.loadtxt('./shift_index_NOTsorted.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amplitude_rescale_train = amplitude_rescale[:split].reshape(X_data.shape[0], 1)\n",
    "amplitude_rescale_valid = amplitude_rescale[split:split+test_valid]\n",
    "amplitude_rescale_test = amplitude_rescale[split+test_valid:]\n",
    "\n",
    "shift_index_train = shift_index[:split].reshape(X_data.shape[0], 1)\n",
    "shift_index_valid = shift_index[split:split+test_valid]\n",
    "shift_index_test = shift_index[split+test_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(X_data[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define network\n",
    "mb_size = 256 # batch size\n",
    "z_dim = 5 # dimension of the latent space\n",
    "h1_dim = 64 # dimension of the hidden layer, i.e. number of nodes\n",
    "h2_dim = 128 # dimension of the hidden layer, i.e. number of nodes\n",
    "h3_dim = 256 # dimension of the hidden layer, i.e. number of nodes\n",
    "h_dim = 50\n",
    "#h4_dim = 600 # dimension of the hidden layer, i.e. number of nodes\n",
    "conv = False # whether the network is going to be convolutional or not\n",
    "conv_1, kernel_1, stride_1 = 32, 11, 2\n",
    "conv_2, kernel_2, stride_2 = 16, 9, 2\n",
    "#conv_3, kernel_3, stride_3 = 32, 16, 2\n",
    "conv_nodes = 200\n",
    "lr = 1e-3  # learning rate\n",
    "\n",
    "def parametric_relu(_x):\n",
    "    #return tf.nn.leaky_relu(_x, alpha=0.8)\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],\n",
    "                       initializer=tf.glorot_normal_initializer,\n",
    "                        dtype=tf.float32)\n",
    "    pos = tf.nn.relu(_x)\n",
    "    neg = alphas * (_x - abs(_x)) * 0.5\n",
    "    return pos + neg\n",
    "\n",
    "act_f = getattr(tf.nn, 'leaky_relu') #parametric_relu #getattr(tf.nn, 'leaky_relu') # activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "c = tf.placeholder(tf.float32, shape=[None, y_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "pf = tf.placeholder(tf.float32)\n",
    "bsize = tf.placeholder(tf.int32)\n",
    "\n",
    "#filter_deconv0 = tf.Variable(tf.random_normal([kernel_3-y_dim, conv_2, conv_3]))\n",
    "filter_deconv1 = tf.Variable(tf.random_normal([kernel_2, conv_1, conv_2]))\n",
    "filter_deconv2 = tf.Variable(tf.random_normal([kernel_1, 1, conv_1]))\n",
    "\n",
    "prior_std_numpy = 1e-3\n",
    "prior_std = tf.constant(prior_std_numpy)\n",
    "# the encoder\n",
    "\n",
    "\n",
    "def Q(X, c):\n",
    "    inputs = X #tf.concat(axis=1, values=[X, c])\n",
    "    if conv:\n",
    "        inputs1 = tf.reshape(inputs, [-1, X_dim, 1]) # reshape to 3D\n",
    "        print(inputs1.shape)\n",
    "        h = tf.layers.conv1d(inputs1, conv_1, kernel_1, strides=stride_1, padding='valid', activation=act_f)\n",
    "        print(h.shape)\n",
    "        h_ = tf.nn.max_pool1d(h, 2, strides=2, padding='VALID')\n",
    "        print(h_.shape)\n",
    "        h2 = tf.layers.conv1d(h_, conv_2, kernel_2, strides=stride_2, padding='valid', activation=act_f)\n",
    "        print(h2.shape)\n",
    "        h2_ = tf.nn.max_pool1d(h2, 2, strides=2, padding='VALID')\n",
    "        print(h2_.shape)\n",
    "        h3 = tf.reshape(h2_, [-1, h2_.shape[1]*h2_.shape[2]])\n",
    "        #h3 = tf.layers.conv1d(h2_, conv_3, kernel_3, strides=stride_3, padding='valid')\n",
    "        print(h3.shape)\n",
    "        h4 = tf.layers.dense(h3, conv_nodes, activation=act_f)\n",
    "        h4_ = tf.concat(axis=1, values=[h4, c])\n",
    "        h5 = tf.layers.dense(h4_, z_dim)\n",
    "        z_mu = tf.reshape(h5, [-1, z_dim])\n",
    "        z_logvar = tf.constant(z_dim*[np.log(prior_std_numpy**2)]) #tf.layers.dense(h4, z_dim)\n",
    "    else:\n",
    "        h3 = tf.layers.dense(inputs, h3_dim, activation=act_f)\n",
    "        h2 = tf.layers.dense(h3, h2_dim, activation=act_f)\n",
    "        h1 = tf.layers.dense(h2, h1_dim, activation=act_f)\n",
    "        h1_ = tf.concat(axis=1, values=[h1, c])\n",
    "        z_mu = tf.layers.dense(h1_, z_dim)\n",
    "        z_logvar = tf.constant(z_dim*[np.log(prior_std_numpy**2)]) #tf.layers.dense(h4, z_dim)\n",
    "    return z_mu, z_logvar\n",
    "\n",
    "def sample_z(mu, log_var):\n",
    "    eps = tf.random_normal(shape=tf.shape(mu))\n",
    "    return mu + tf.exp(log_var / 2) * eps\n",
    "    #return mu + std * eps\n",
    "    #return mu\n",
    "\n",
    "# the decoder\n",
    "\n",
    "def P(zed, c, bsize):\n",
    "    inputs = tf.concat(axis=1, values=[zed, c])\n",
    "    if conv:\n",
    "        h5 = tf.layers.dense(inputs, conv_nodes, activation=act_f)\n",
    "        print(h5.shape)\n",
    "        h4_ = tf.layers.dense(h5, 29*conv_2, activation=act_f)\n",
    "        print(h4_.shape)\n",
    "        h4_res = tf.reshape(h4_, [-1, 29, conv_2])\n",
    "        print(h4_res.shape)\n",
    "        h4 = tf.keras.layers.UpSampling1D(2)(h4_res)\n",
    "        print(h4.shape)\n",
    "        h3 = tf.nn.conv1d_transpose(h4, filter_deconv1, output_shape=[bsize, 123, conv_1], strides=stride_2, padding='VALID')\n",
    "        h3_ = act_f(h3)\n",
    "        print(h3_.shape)\n",
    "        h3__ = tf.keras.layers.UpSampling1D(2)(h3_)\n",
    "        print(h3__.shape)\n",
    "        h2 = tf.nn.conv1d_transpose(h3__, filter_deconv2, output_shape=[bsize, X_dim, 1], strides=stride_1, padding='VALID')\n",
    "        print(h2.shape)\n",
    "        h1 = tf.reshape(h2, [-1, X_dim])\n",
    "        logits = h1\n",
    "    else:\n",
    "        #print('Not conv')\n",
    "        h1 = tf.layers.dense(inputs, h1_dim, activation=act_f)\n",
    "        h2 = tf.layers.dense(h1, h2_dim, activation=act_f)\n",
    "        h3 = tf.layers.dense(h2, h3_dim, activation=act_f)\n",
    "        logits = tf.layers.dense(h3, X_dim)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-f404d83f1467>:38: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/alessio/anaconda3/lib/python2.7/site-packages/tensorflow_core/python/layers/core.py:187: apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/alessio/anaconda3/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "z_mu, z_logvar = Q(X, c)\n",
    "z_sample = sample_z(z_mu, z_logvar)\n",
    "\n",
    "decoder = tf.make_template('decoder', P)\n",
    "# sampling from random z\n",
    "X_samples_rand = decoder(z, c, bsize)\n",
    "X_samples = decoder(z_sample, c, bsize)\n",
    "\n",
    "# E[log P(X|z,c)]\n",
    "recon_loss = tf.keras.losses.MSE(X, X_samples)#tf.losses.mean_squared_error(X, logits)\n",
    "#tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=X), 1)\n",
    "\n",
    "# D_KL(Q(z|X,c) || P(z|X,c)); calculate in closed form as both dist. are Gaussian\n",
    "kl_loss = 0.5 * tf.reduce_mean(tf.exp(z_logvar)/(prior_std**2) + z_mu**2/(prior_std**2) - 1. - z_logvar + tf.log(prior_std**2), 1) #tf.constant(0.0) #0.5 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1)\n",
    "\n",
    "# VAE loss\n",
    "vae_loss = tf.reduce_mean(recon_loss + pf*kl_loss)\n",
    "\n",
    "solver = tf.train.AdamOptimizer().minimize(vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# losses = []\n",
    "# valid_losses = []\n",
    "\n",
    "# training_CVAE_R2 = []\n",
    "# validation_CVAE_R2 = []\n",
    "# test_CVAE_R2 = []\n",
    "\n",
    "# n_epochs = 10000 # number of epochs\n",
    "# best_loss = 1e8\n",
    "# stopping_step = 0\n",
    "# patience = 100\n",
    "# prefac = 1.0 \n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "    \n",
    "#     # training\n",
    "#     random_perm = np.random.permutation(X_data.shape[0])\n",
    "#     mini_batch_index = 0\n",
    "#     while True:\n",
    "#         indices = random_perm[mini_batch_index:mini_batch_index+mb_size]\n",
    "#         sess.run(solver, feed_dict={X: X_data[indices], c: y_data[indices], bsize: X_data[indices].shape[0], pf: prefac})\n",
    "#         mini_batch_index += mb_size\n",
    "#         if mini_batch_index >= X_data.shape[0]:\n",
    "#             break\n",
    "\n",
    "#     # metrics\n",
    "#     clear_output(wait=True)\n",
    "\n",
    "#     loss, rec, kl = sess.run([vae_loss, recon_loss, kl_loss], feed_dict={X: X_data[indices], c: y_data[indices], bsize: X_data[indices].shape[0], pf: prefac})\n",
    "#     valid_loss, valid_rec, valid_kl = sess.run([vae_loss, recon_loss, kl_loss], feed_dict={X: X_data_test, c: y_data_test, bsize: test_valid, pf: prefac})\n",
    "\n",
    "#     losses.append(loss)\n",
    "#     #losses_rec.append(np.mean(rec))\n",
    "#     #losses_kl.append(np.mean(kl))\n",
    "\n",
    "#     valid_losses.append(valid_loss)\n",
    "#     #valid_losses_rec.append(np.mean(valid_rec))\n",
    "#     #valid_losses_kl.append(np.mean(kl))\n",
    "\n",
    "#     print('Epoch: {}'.format(epoch))\n",
    "#     print('Loss: {:.4}'.format(np.mean(loss)))\n",
    "#     print('Validation loss: {:.4}'.format(np.mean(valid_loss)))\n",
    "#     #print('KL loss: {:.4}'.format(np.mean(kl)))\n",
    "#     #print('Validation KL loss: {:.4}'.format(np.mean(valid_kl)))\n",
    "\n",
    "#     #X_R2, y_R2, _ = next_batch(X_data, y_data, split)\n",
    "#     #prediction_training = sess.run(X_samples_rand, feed_dict={z: prior_std_numpy*np.random.randn(split, z_dim), c: y_R2, bsize: split})\n",
    "#     #calculate_R2(X_R2, prediction_training, 'Training CVAE: ', training_CVAE_R2)\n",
    "#     #prediction_validation = sess.run(X_samples_rand, feed_dict={z: prior_std_numpy*np.random.randn(test_valid, z_dim), c: y_data_valid, bsize: test_valid})\n",
    "#     #calculate_R2(X_data_valid, prediction_validation, 'Validation CVAE: ', validation_CVAE_R2)\n",
    "#     #prediction_test = sess.run(X_samples_rand, feed_dict={z: prior_std_numpy*np.random.randn(test_valid, z_dim), c: y_data_test, bsize: test_valid})\n",
    "#     #calculate_R2(X_data_test, prediction_test, 'Test CVAE: ', test_CVAE_R2)    \n",
    "\n",
    "#     loss_value = np.mean(valid_loss)\n",
    "#     if loss_value < best_loss:\n",
    "#         stopping_step = 0\n",
    "#         best_loss = loss_value\n",
    "#         save_path = saver.save(sess, f\"./saved_models_iso_CVAE/best_model.ckpt\")\n",
    "#     else:\n",
    "#         stopping_step += 1\n",
    "#         print(f'Early stopping: {stopping_step}/{patience}')\n",
    "#     if stopping_step >= patience:\n",
    "#         print(f'Patience limit reached at epoch {epoch}')\n",
    "#         break\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(10, 4))\n",
    "# plt.plot(losses, label='Training')\n",
    "# plt.plot(valid_losses, label='Validation')\n",
    "# #plt.plot(losses_rec, label='rec')        \n",
    "# #plt.plot(losses_kl, label='kl')\n",
    "# #plt.plot(losses_rec, label='rec test')        \n",
    "# #plt.plot(valid_losses_rec, label='validation rec')        \n",
    "# #plt.plot(valid_losses_kl, label='validation kl')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_models_iso_CVAE/best_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# load best AE model\n",
    "#print(f'Best model was model {len(losses) - patience}')\n",
    "load_path = (\"./saved_models_iso_CVAE/best_model.ckpt\")\n",
    "saver.restore(sess, load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # fit GP to data rescaling\n",
    "# # amplitude\n",
    "# kern = GPy.kern.Matern32(4,ARD=True)\n",
    "# n = GPy.models.GPRegression(y_data, amplitude_rescale_train, kern)\n",
    "# n.optimize(messages=True, max_f_eval = 1000)\n",
    "\n",
    "# # GP softwares should have regularisation techniques to prevent overfitting\n",
    "# # already implemented. This way, we can just look at training performance, \n",
    "# # as in, that training converged, to be quite sure it did not overfit.\n",
    "# # As a confirmation of this, we print train and validation R2 scores, \n",
    "# # and check they are compatible. If they are not, then overfit happened,\n",
    "# # maybe because the size sample was too small and regularisation techniques failed.\n",
    "# # If they are on par, then there is good generalisation and no signs of overfitting.\n",
    "# # We note that overfitting may have happened anyway, as this is just a necessary, \n",
    "# # but not sufficient, condition. However, we expect the algorithm to be robust enough\n",
    "# # when trained on these representative data that by just looking at train and\n",
    "# # validation R2 values we can exclude overfitting.\n",
    "\n",
    "# y_pred_train = n.predict(y_data)[0]\n",
    "# y_pred_train = y_pred_train[:, 0]\n",
    "# calculate_R2(amplitude_rescale_train.flatten(), y_pred_train, 'Amplitude R2 train: ', [])\n",
    "\n",
    "# y_pred_valid = n.predict(y_data_valid)[0]\n",
    "# y_pred_valid = y_pred_valid[:, 0]\n",
    "# calculate_R2(amplitude_rescale_valid, y_pred_valid, 'Amplitude R2 validation: ', [])\n",
    "\n",
    "# # this is to be used later\n",
    "# y_pred_test = n.predict(y_data_test)[0]\n",
    "# y_pred_test = y_pred_test[:, 0]\n",
    "\n",
    "# #plt.plot(amplitude_rescale_test, color='blue')\n",
    "# #plt.show()\n",
    "# #plt.plot(y_pred_test, color='red')\n",
    "# #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # time shift\n",
    "# ker = GPy.kern.Matern32(4,ARD=True)\n",
    "# m = GPy.models.GPRegression(y_data,shift_index_train,ker)\n",
    "# m.optimize(messages=True,max_f_eval = 1000)\n",
    "\n",
    "# y_pred_train_2 = m.predict(y_data)[0]\n",
    "# y_pred_train_2 = y_pred_train_2[:, 0]\n",
    "# calculate_R2(shift_index_train.flatten(), y_pred_train_2, 'Time shift R2 training: ', [])\n",
    "\n",
    "# y_pred_valid_2 = m.predict(y_data_valid)[0]\n",
    "# y_pred_valid_2 = y_pred_valid_2[:, 0]\n",
    "# calculate_R2(shift_index_valid, y_pred_valid_2, 'Time shift R2 validation: ', [])\n",
    "\n",
    "# y_pred_test_2 = m.predict(y_data_test)[0]\n",
    "# y_pred_test_2 = y_pred_test_2[:, 0]\n",
    "\n",
    "# #plt.plot(shift_index_test, color='blue')\n",
    "# #plt.show()\n",
    "# #plt.plot(y_pred_test_2, color='red')\n",
    "# #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction_testing = sess.run(X_samples_rand, feed_dict={z: prior_std_numpy*np.random.randn(test_valid, z_dim), c: y_data_test, bsize: test_valid})\n",
    "    \n",
    "# prediction_testing = np.multiply(prediction_testing, np.repeat(1/y_pred_test, X_dim).reshape(-1, X_dim))\n",
    "# for index_seism in range(test_valid):\n",
    "#     prediction_testing[index_seism] = shift(prediction_testing[index_seism], -y_pred_test_2[index_seism], cval=0.)\n",
    "\n",
    "# # retrieve the unprocessed data back\n",
    "# X_data_preprocessed = preprocess_seismo(X_data_, split, log=False, std=False, rescale=False, rescale_onlyamp=False)\n",
    "# X_data_test = X_data_preprocessed[split+test_valid:]\n",
    "\n",
    "# calculate_R2(X_data_test, prediction_testing, 'Final R2 testing: ', [])\n",
    "\n",
    "# #for i in range(5):\n",
    "# #    plt.plot(X_data_test[i], color='blue')\n",
    "# #    plt.plot(prediction_testing[i], color='red')\n",
    "# #    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/alessio/anaconda3/lib/python2.7/site-packages/paramz/parameterized.py:61: RuntimeWarning:Don't forget to initialize by self.initialize_parameter()!\n"
     ]
    }
   ],
   "source": [
    "ker = GPy.kern.Matern32(4,ARD=True) \n",
    "m_load = GPy.models.GPRegression(y_data, amplitude_rescale_train, ker, initialize=False)\n",
    "m_load.update_model(False) # do not call the underlying expensive algebra on load\n",
    "m_load.initialize_parameter() # Initialize the parameters (connect the parameters up)\n",
    "m_load[:] = np.load('./saved_models_iso_CVAE/GPmodel_amplitude.npy') # Load the parameters\n",
    "m_load.update_model(True) # Call the algebra only once\n",
    "\n",
    "kern = GPy.kern.Matern32(4,ARD=True) \n",
    "n_load = GPy.models.GPRegression(y_data, shift_index_train, kern, initialize=False)\n",
    "n_load.update_model(False) # do not call the underlying expensive algebra on load\n",
    "n_load.initialize_parameter() # Initialize the parameters (connect the parameters up)\n",
    "n_load[:] = np.load('./saved_models_iso_CVAE/GPmodel_time.npy') # Load the parameters\n",
    "n_load.update_model(True) # Call the algebra only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('timeinf', 0.06417679786682129)\n"
     ]
    }
   ],
   "source": [
    "start_time_inference = time.time()\n",
    "\n",
    "coordinate = np.array([31,25,158])\n",
    "\n",
    "shifted = coordinate - np.array([41,41,244])\n",
    "distances = np.linalg.norm(shifted)\n",
    "new_coords = np.zeros(4)\n",
    "new_coords[:3] = shifted\n",
    "new_coords[-1] = distances\n",
    "new_coords = (new_coords - meancoords)/stdcoords\n",
    "new_coords = new_coords.reshape((1,y_dim))\n",
    "\n",
    "prediction_testing = sess.run(X_samples_rand, feed_dict={z: prior_std_numpy*np.random.randn(1, z_dim), c: new_coords})\n",
    "y_pred_test = m_load.predict(new_coords)[0]\n",
    "y_pred_test = y_pred_test[:, 0]\n",
    "y_pred_test_2 = n_load.predict(new_coords)[0]\n",
    "y_pred_test_2 = y_pred_test_2[:, 0]\n",
    "prediction_testing = np.multiply(prediction_testing, np.repeat(1/y_pred_test, X_dim).reshape(-1, X_dim))\n",
    "prediction_testing = shift(prediction_testing[0], -y_pred_test_2, cval=0.)\n",
    "\n",
    "timeinf = time.time() - start_time_inference\n",
    "print(\"timeinf\", timeinf)\n",
    "np.save(\"CVAE_inftime.npy\", timeinf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(prediction_testing))\n",
    "plt.close()\n",
    "plt.plot(prediction_testing, linewidth=3, color='black')\n",
    "plt.savefig('seismoerin.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # total time, to be quoted in the paper\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
