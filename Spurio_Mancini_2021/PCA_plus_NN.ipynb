{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import clear_output  # this has to be removed if used on a single script\n",
    "import random\n",
    "from preprocess import preprocess_seismo, preprocess_coord\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import GPy\n",
    "import time\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(x, y, mb_size):\n",
    "    idx = np.arange(len(x), dtype = np.int64)\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:mb_size]\n",
    "    return x[idx], y[idx], idx\n",
    "\n",
    "def plot(x, index=0):\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.plot(x, label='Reconstructed')\n",
    "    plt.plot(X_data[index], label='Real')\n",
    "    plt.legend()\n",
    "\n",
    "def plot_test(x, index=0):\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.plot(x, label='Reconstructed')\n",
    "    plt.plot(X_data_test[index], label='Real')\n",
    "    plt.legend()\n",
    "\n",
    "def calculate_R2(original, prediction, label, store):\n",
    "    AM = original.mean()\n",
    "    BM = prediction.mean()\n",
    "    c_vect = (original-AM)*(prediction-BM)\n",
    "    d_vect = (original-AM)**2\n",
    "    e_vect = (prediction-BM)**2\n",
    "    r_out = np.sum(c_vect)/float(np.sqrt(np.sum(d_vect)*np.sum(e_vect)))\n",
    "    print(label+str(r_out))\n",
    "    store.append(r_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define number of time components in seismograms, number of coordinates, train/test split and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dim = 501 # size of the seismograms\n",
    "y_dim = 4\n",
    "# load data\n",
    "split = 2000\n",
    "test_valid = 1000\n",
    "X_data_ = np.load('./seismograms_4000seismo_ISO.npy')[:, :X_dim]\n",
    "y_data_ = np.load('./coordinates_4000seismo_ISO.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess coordinates\n",
    "y_data_preprocessed, meancoords, stdcoords = preprocess_coord(y_data_, split=split, test_valid=test_valid, sort=False, std=True)\n",
    "y_data = y_data_preprocessed[:split]\n",
    "y_data_valid =  y_data_preprocessed[split:split+test_valid]\n",
    "y_data_test =  y_data_preprocessed[split+test_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess seismograms\n",
    "X_data_preprocessed = preprocess_seismo(X_data_, split, log=False, std=False, rescale=True, rescale_onlyamp=False)\n",
    "X_data = X_data_preprocessed[:split]\n",
    "X_data_valid =  X_data_preprocessed[split:split+test_valid]\n",
    "X_data_test =  X_data_preprocessed[split+test_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save rescaled amplitudes and time shift indices on file\n",
    "amplitude_rescale = np.loadtxt('./amplitude_rescale_NOTsorted.txt')\n",
    "shift_index = np.loadtxt('./shift_index_NOTsorted.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardise data before PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we standardise data to apply PCA\n",
    "meanseismo = np.mean(X_data, axis=0)\n",
    "stdseismo = np.std(X_data, axis=0)\n",
    "X_data = (X_data - meanseismo)/stdseismo\n",
    "X_data_valid = (X_data_valid - meanseismo)/stdseismo\n",
    "X_data_test = (X_data_test - meanseismo)/stdseismo\n",
    "\n",
    "# number of PCA components\n",
    "selected_comp = 20\n",
    "\n",
    "pca = PCA(n_components=selected_comp)\n",
    "pca.fit(X_data)\n",
    "basis = pca.components_\n",
    "dominantseismo_train = pca.transform(X_data)\n",
    "dominantseismo_valid = pca.transform(X_data_valid)\n",
    "#dominantseismo_test = pca.transform(X_data_test)\n",
    "        \n",
    "seismo = np.matmul(dominantseismo_train, basis)\n",
    "seismo_valid = np.matmul(dominantseismo_valid, basis)\n",
    "#seismo_test = dominantseismo_test @ basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise the PCA components before feeding them into NN\n",
    "mean = np.mean(dominantseismo_train)\n",
    "std = np.std(dominantseismo_train)\n",
    "dominantseismo_train = (dominantseismo_train - mean)/std\n",
    "dominantseismo_valid = (dominantseismo_valid - mean)/std\n",
    "#dominantseismo_test = (dominantseismo_test - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "mb_size = 256 # batch size\n",
    "act_f = getattr(tf.nn, 'leaky_relu') #parametric_relu #getattr(tf.nn, 'leaky_relu') # activation function\n",
    "lr = 1e-4\n",
    "\n",
    "tf.reset_default_graph() #this will eliminate the variables we restored\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "c = tf.placeholder(tf.float32, shape=[None, y_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, selected_comp])\n",
    "bsize = tf.placeholder(tf.int32)\n",
    "\n",
    "h_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_P(coord):\n",
    "    h1 = tf.layers.dense(coord, h_dim, activation=act_f)\n",
    "    h2 = tf.layers.dense(h1, h_dim, activation=act_f)\n",
    "    h3 = tf.layers.dense(h2, h_dim, activation=act_f)\n",
    "    logits = tf.layers.dense(h3, selected_comp)\n",
    "    return logits\n",
    "\n",
    "z_nn_samples = latent_P(c)\n",
    "\n",
    "# reconstruction loss\n",
    "recon_loss_nn = tf.keras.losses.MSE(z, z_nn_samples)\n",
    "\n",
    "# loss\n",
    "loss_nn = tf.reduce_mean(recon_loss_nn)\n",
    "\n",
    "solver_nn = tf.train.AdamOptimizer().minimize(loss_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_rec_nn = []\n",
    "valid_losses_rec_nn = []\n",
    "\n",
    "training_nn_R2 = []\n",
    "validation_nn_R2 = []\n",
    "test_nn_R2 = []\n",
    "\n",
    "n_epochs = 100 # number of epochs\n",
    "best_loss = 1e8\n",
    "stopping_step = 0\n",
    "patience = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # train\n",
    "    random_perm = np.random.permutation(dominantseismo_train.shape[0])\n",
    "    mini_batch_index = 0\n",
    "    while True:\n",
    "        indices = random_perm[mini_batch_index:mini_batch_index+mb_size]\n",
    "        sess.run(solver_nn, feed_dict={z: dominantseismo_train[indices], c: y_data[indices]})\n",
    "        mini_batch_index += mb_size\n",
    "        if mini_batch_index >= X_data.shape[0]:\n",
    "            break\n",
    "\n",
    "    # metrics\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    rec = sess.run(recon_loss_nn, feed_dict={z: dominantseismo_train, c: y_data})\n",
    "    valid_rec = sess.run(recon_loss_nn, feed_dict={z: dominantseismo_valid, c: y_data_valid})\n",
    "\n",
    "    losses_rec_nn.append(np.mean(rec))\n",
    "    valid_losses_rec_nn.append(np.mean(valid_rec))\n",
    "\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    print('Rec loss: {:.4}'.format(np.mean(rec)))\n",
    "    print('Validation rec loss: {:.4}'.format(np.mean(valid_rec)))\n",
    "        \n",
    "    # we only look at the z reconstruction, as we integrate PCA later on\n",
    "    #X_R2, y_R2, _ = next_batch(dominantseismo_train, y_data, split)\n",
    "    #prediction_training = sess.run(z_nn_samples, feed_dict={c: y_R2})\n",
    "    #calculate_R2(X_R2, prediction_training, 'Training NN ', training_nn_R2)\n",
    "\n",
    "    #prediction_valid = sess.run(z_nn_samples, feed_dict={c: y_data_valid})\n",
    "    #calculate_R2(dominantseismo_valid, prediction_valid, 'Validation NN: ', validation_nn_R2)\n",
    "\n",
    "    #prediction_test = sess.run(z_nn_samples, feed_dict={c: y_data_test})\n",
    "    #calculate_R2(dominantseismo_test, prediction_test, 'Test NN: ', test_nn_R2)\n",
    "\n",
    "    loss_value = np.mean(valid_rec)\n",
    "    if loss_value < best_loss:\n",
    "        stopping_step = 0\n",
    "        best_loss = loss_value\n",
    "        save_path = saver.save(sess, \"./PCA_plus_NN/best_model.ckpt\")\n",
    "    else:\n",
    "        stopping_step += 1\n",
    "        print('Early stopping: {}/{}'.format(stopping_step, patience))\n",
    "    if stopping_step >= patience:\n",
    "        print('Patience limit reached at epoch {}'.format(epoch))\n",
    "        break\n",
    "        \n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses_rec_nn, label='Training')        \n",
    "plt.plot(valid_losses_rec_nn, label='Validation')        \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model\n",
    "#print('Best model was model {}'.format(len(losses_rec_nn) - patience))\n",
    "load_path = (\"./saved_models_iso_PCAplusNN/best_model.ckpt\")\n",
    "saver.restore(sess, load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP training for Amplitude and Time shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape for GP training\n",
    "amplitude_rescale_train = amplitude_rescale[:split].reshape(X_data.shape[0], 1)\n",
    "amplitude_rescale_valid = amplitude_rescale[split:split+test_valid]\n",
    "amplitude_rescale_test = amplitude_rescale[split+test_valid:]\n",
    "\n",
    "shift_index_train = shift_index[:split].reshape(X_data.shape[0], 1)\n",
    "shift_index_valid = shift_index[split:split+test_valid]\n",
    "shift_index_test = shift_index[split+test_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit GP to data rescaling\n",
    "# amplitude\n",
    "kern = GPy.kern.Matern32(4,ARD=True)\n",
    "n = GPy.models.GPRegression(y_data, amplitude_rescale_train, kern)\n",
    "n.optimize(messages=True, max_f_eval = 1000)\n",
    "\n",
    "y_pred_train = n.predict(y_data)[0]\n",
    "y_pred_train = y_pred_train[:, 0]\n",
    "calculate_R2(amplitude_rescale_train.flatten(), y_pred_train, 'Amplitude R2 train: ', [])\n",
    "\n",
    "y_pred_valid = n.predict(y_data_valid)[0]\n",
    "y_pred_valid = y_pred_valid[:, 0]\n",
    "calculate_R2(amplitude_rescale_valid, y_pred_valid, 'Amplitude R2 validation: ', [])\n",
    "\n",
    "# this is to be used later\n",
    "y_pred_test = n.predict(y_data_test)[0]\n",
    "y_pred_test = y_pred_test[:, 0]\n",
    "\n",
    "#plt.plot(amplitude_rescale_test, color='blue')\n",
    "#plt.show()\n",
    "#plt.plot(y_pred_test, color='red')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time shift\n",
    "ker = GPy.kern.Matern32(4,ARD=True)\n",
    "m = GPy.models.GPRegression(y_data,shift_index_train,ker)\n",
    "m.optimize(messages=True,max_f_eval = 1000)\n",
    "\n",
    "y_pred_train_2 = m.predict(y_data)[0]\n",
    "y_pred_train_2 = y_pred_train_2[:, 0]\n",
    "calculate_R2(shift_index_train.flatten(), y_pred_train_2, 'Time shift R2 training: ', [])\n",
    "\n",
    "y_pred_valid_2 = m.predict(y_data_valid)[0]\n",
    "y_pred_valid_2 = y_pred_valid_2[:, 0]\n",
    "calculate_R2(shift_index_valid, y_pred_valid_2, 'Time shift R2 validation: ', [])\n",
    "\n",
    "y_pred_test_2 = m.predict(y_data_test)[0]\n",
    "y_pred_test_2 = y_pred_test_2[:, 0]\n",
    "\n",
    "#plt.plot(shift_index_test, color='blue')\n",
    "#plt.show()\n",
    "#plt.plot(y_pred_test_2, color='red')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain reconstructed seismo for the test set\n",
    "prediction_z_test = sess.run(z_nn_samples, feed_dict={c: y_data_test})\n",
    "prediction_z_test = prediction_z_test*std + mean\n",
    "prediction_testing = prediction_z_test@basis\n",
    "prediction_testing = prediction_testing*stdseismo+meanseismo\n",
    "\n",
    "prediction_testing = np.multiply(prediction_testing, np.repeat(1/y_pred_test, X_dim).reshape(-1, X_dim))\n",
    "\n",
    "for index_seism in range(test_valid):\n",
    "    prediction_testing[index_seism] = shift(prediction_testing[index_seism], -y_pred_test_2[index_seism], cval=0.)\n",
    "\n",
    "# retrieve the unprocessed data back\n",
    "X_data_preprocessed = preprocess_seismo(X_data_, split, log=False, std=False, rescale=False, rescale_onlyamp=False)\n",
    "X_data_test = X_data_preprocessed[split+test_valid:]\n",
    "\n",
    "calculate_R2(X_data_test, prediction_testing, 'Final R2 testing: ', [])\n",
    "\n",
    "#for i in range(5):\n",
    "#    plt.plot(X_data_test[i], color='blue')\n",
    "#    plt.plot(prediction_testing[i], color='red')\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ker = GPy.kern.Matern32(4,ARD=True) \n",
    "m_load = GPy.models.GPRegression(y_data, amplitude_rescale_train, ker, initialize=False)\n",
    "m_load.update_model(False) # do not call the underlying expensive algebra on load\n",
    "m_load.initialize_parameter() # Initialize the parameters (connect the parameters up)\n",
    "m_load[:] = np.load('./saved_models_iso_PCAplusNN/GPmodel_amplitude.npy') # Load the parameters\n",
    "m_load.update_model(True) # Call the algebra only once\n",
    "\n",
    "kern = GPy.kern.Matern32(4,ARD=True) \n",
    "n_load = GPy.models.GPRegression(y_data, shift_index_train, kern, initialize=False)\n",
    "n_load.update_model(False) # do not call the underlying expensive algebra on load\n",
    "n_load.initialize_parameter() # Initialize the parameters (connect the parameters up)\n",
    "n_load[:] = np.load('./saved_models_iso_PCAplusNN/GPmodel_time.npy') # Load the parameters\n",
    "n_load.update_model(True) # Call the algebra only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_inference = time.time()\n",
    "\n",
    "coordinate = np.array([31,25,158])\n",
    "\n",
    "shifted = coordinate - np.array([41,41,244])\n",
    "distances = np.linalg.norm(shifted)\n",
    "new_coords = np.zeros(4)\n",
    "new_coords[:3] = shifted\n",
    "new_coords[-1] = distances\n",
    "new_coords = (new_coords - meancoords)/stdcoords\n",
    "new_coords = new_coords.reshape((1,y_dim))\n",
    "\n",
    "prediction_z_test = sess.run(z_nn_samples, feed_dict={c: new_coords})\n",
    "prediction_z_test = prediction_z_test*std + mean\n",
    "prediction_testing = np.matmul(prediction_z_test,basis)\n",
    "prediction_testing = prediction_testing*stdseismo+meanseismo\n",
    "y_pred_test = m_load.predict(new_coords)[0]\n",
    "y_pred_test = y_pred_test[:, 0]\n",
    "y_pred_test_2 = n_load.predict(new_coords)[0]\n",
    "y_pred_test_2 = y_pred_test_2[:, 0]\n",
    "prediction_testing = np.multiply(prediction_testing, np.repeat(1/y_pred_test, X_dim).reshape(-1, X_dim))\n",
    "prediction_testing = shift(prediction_testing[0], -y_pred_test_2, cval=0.)\n",
    "\n",
    "timeinf = time.time() - start_time_inference\n",
    "print(\"timeinf\", timeinf)\n",
    "np.save(\"PCA_plus_NN_inftime.npy\", timeinf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
