{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from IPython.display import clear_output  # this has to be removed if used on a single script\n",
    "import random\n",
    "from preprocess import preprocess_seismo, preprocess_coord\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import GPy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(x, y, mb_size):\n",
    "    idx = np.arange(len(x), dtype = np.int64)\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:mb_size]\n",
    "    return x[idx], y[idx], idx\n",
    "\n",
    "def plot(x, index=0):\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.plot(x, label='Reconstructed')\n",
    "    plt.plot(X_data[index], label='Real')\n",
    "    plt.legend()\n",
    "\n",
    "def plot_test(x, index=0):\n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    plt.plot(x, label='Reconstructed')\n",
    "    plt.plot(X_data_test[index], label='Real')\n",
    "    plt.legend()\n",
    "\n",
    "def calculate_R2(original, prediction, label, store, component=None):\n",
    "    AM = original.mean()\n",
    "    BM = prediction.mean()\n",
    "    c_vect = (original-AM)*(prediction-BM)\n",
    "    d_vect = (original-AM)**2\n",
    "    e_vect = (prediction-BM)**2\n",
    "    r_out = np.sum(c_vect)/float(np.sqrt(np.sum(d_vect)*np.sum(e_vect)))\n",
    "    if component == None:\n",
    "        print(label+str(r_out))\n",
    "    else:\n",
    "        print(str(component)+label+str(r_out))\n",
    "    store.append(r_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define number of time components in seismograms, number of coordinates, train/test split and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dim = 501 # size of the seismograms\n",
    "y_dim = 4\n",
    "# load data\n",
    "split = 2000\n",
    "test_valid = 1000\n",
    "X_data_ = np.load('./seismograms_4000seismo_ISO.npy')[:, :X_dim]\n",
    "y_data_ = np.load('./coordinates_4000seismo_ISO.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess coordinates\n",
    "y_data_preprocessed, meancoords, stdcoords = preprocess_coord(y_data_, split=split, test_valid=test_valid, sort=False, std=True)\n",
    "y_data = y_data_preprocessed[:split]\n",
    "y_data_valid =  y_data_preprocessed[split:split+test_valid]\n",
    "y_data_test =  y_data_preprocessed[split+test_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess seismograms\n",
    "X_data_preprocessed = preprocess_seismo(X_data_, split, log=False, std=False, rescale=True, rescale_onlyamp=False)\n",
    "X_data = X_data_preprocessed[:split]\n",
    "X_data_valid =  X_data_preprocessed[split:split+test_valid]\n",
    "X_data_test =  X_data_preprocessed[split+test_valid:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "mb_size = 256 # batch size\n",
    "z_dim = 5 # dimension of the latent space\n",
    "h1_dim = 64 # dimension of the hidden layer, i.e. number of nodes\n",
    "h2_dim = 128 # dimension of the hidden layer, i.e. number of nodes\n",
    "h3_dim = 256 # dimension of the hidden layer, i.e. number of nodes\n",
    "h_dim = 50\n",
    "#h4_dim = 600 # dimension of the hidden layer, i.e. number of nodes\n",
    "conv = False # whether the network is going to be convolutional or not\n",
    "conv_1, kernel_1, stride_1 = 32, 11, 2\n",
    "conv_2, kernel_2, stride_2 = 16, 9, 2\n",
    "#conv_3, kernel_3, stride_3 = 32, 16, 2\n",
    "conv_nodes = 200\n",
    "lr = 1e-3  # learning rate\n",
    "\n",
    "def parametric_relu(_x):\n",
    "    #return tf.nn.leaky_relu(_x, alpha=0.8)\n",
    "    alphas = tf.get_variable('alpha', _x.get_shape()[-1],\n",
    "                       initializer=tf.glorot_normal_initializer,\n",
    "                        dtype=tf.float32)\n",
    "    pos = tf.nn.relu(_x)\n",
    "    neg = alphas * (_x - abs(_x)) * 0.5\n",
    "    return pos + neg\n",
    "\n",
    "act_f = getattr(tf.nn, 'leaky_relu') #parametric_relu #getattr(tf.nn, 'leaky_relu') # activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #this will eliminate the variables we restored\n",
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "c = tf.placeholder(tf.float32, shape=[None, y_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])\n",
    "bsize = tf.placeholder(tf.int32)\n",
    "\n",
    "#filter_deconv0 = tf.Variable(tf.random_normal([kernel_3-y_dim, conv_2, conv_3]))\n",
    "filter_deconv1 = tf.Variable(tf.random_normal([kernel_2, conv_1, conv_2]))\n",
    "filter_deconv2 = tf.Variable(tf.random_normal([kernel_1, 1, conv_1]))\n",
    "\n",
    "\n",
    "# the encoder\n",
    "def Q(X):\n",
    "    inputs = X #tf.concat(axis=1, values=[X, c])\n",
    "    if conv:\n",
    "        inputs1 = tf.reshape(inputs, [-1, X_dim, 1]) # reshape to 3D\n",
    "        print(inputs1.shape)\n",
    "        h = tf.layers.conv1d(inputs1, conv_1, kernel_1, strides=stride_1, padding='valid', activation=act_f)\n",
    "        print(h.shape)\n",
    "        h_ = tf.nn.max_pool1d(h, 2, strides=2, padding='VALID')\n",
    "        print(h_.shape)\n",
    "        h2 = tf.layers.conv1d(h_, conv_2, kernel_2, strides=stride_2, padding='valid', activation=act_f)\n",
    "        print(h2.shape)\n",
    "        h2_ = tf.nn.max_pool1d(h2, 2, strides=2, padding='VALID')\n",
    "        print(h2_.shape)\n",
    "        h3 = tf.reshape(h2_, [-1, h2_.shape[1]*h2_.shape[2]])\n",
    "        #h3 = tf.layers.conv1d(h2_, conv_3, kernel_3, strides=stride_3, padding='valid')\n",
    "        print(h3.shape)\n",
    "        h4 = tf.layers.dense(h3, conv_nodes, activation=act_f)\n",
    "        h5 = tf.layers.dense(h4, z_dim)\n",
    "        z_mu = tf.reshape(h5, [-1, z_dim])\n",
    "        z_logvar = tf.constant(0.0) #tf.constant(z_dim*[np.log(prior_std**2)]) #tf.layers.dense(h4, z_dim)\n",
    "    else:\n",
    "        h3 = tf.layers.dense(inputs, h3_dim, activation=act_f)\n",
    "        h2 = tf.layers.dense(h3, h2_dim, activation=act_f)\n",
    "        h1 = tf.layers.dense(h2, h1_dim, activation=act_f)\n",
    "        z_mu = tf.layers.dense(h1, z_dim)\n",
    "        z_logvar = tf.constant(0.0) # tf.constant(z_dim*[np.log(prior_std**2)]) #tf.layers.dense(h4, z_dim)\n",
    "    return z_mu, z_logvar\n",
    "\n",
    "\n",
    "# the decoder\n",
    "def P(zed, bsize):\n",
    "    inputs = zed #tf.concat(axis=1, values=[zed, c])\n",
    "    if conv:\n",
    "        h5 = tf.layers.dense(inputs, conv_nodes, activation=act_f)\n",
    "        print(h5.shape)\n",
    "        h4_ = tf.layers.dense(h5, 29*conv_2, activation=act_f)\n",
    "        print(h4_.shape)\n",
    "        h4_res = tf.reshape(h4_, [-1, 29, conv_2])\n",
    "        print(h4_res.shape)\n",
    "        h4 = tf.keras.layers.UpSampling1D(2)(h4_res)\n",
    "        print(h4.shape)\n",
    "        h3 = tf.nn.conv1d_transpose(h4, filter_deconv1, output_shape=[bsize, 123, conv_1], strides=stride_2, padding='VALID')\n",
    "        h3_ = act_f(h3)\n",
    "        print(h3_.shape)\n",
    "        h3__ = tf.keras.layers.UpSampling1D(2)(h3_)\n",
    "        print(h3__.shape)\n",
    "        h2 = tf.nn.conv1d_transpose(h3__, filter_deconv2, output_shape=[bsize, X_dim, 1], strides=stride_1, padding='VALID')\n",
    "        print(h2.shape)\n",
    "        h1 = tf.reshape(h2, [-1, X_dim])\n",
    "        logits = h1\n",
    "    else:\n",
    "        #print('Not conv')\n",
    "        h1 = tf.layers.dense(inputs, h1_dim, activation=act_f)\n",
    "        h2 = tf.layers.dense(h1, h2_dim, activation=act_f)\n",
    "        h3 = tf.layers.dense(h2, h3_dim, activation=act_f)\n",
    "        logits = tf.layers.dense(h3, X_dim)\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_sample, _ = Q(X)\n",
    "#z_sample = sample_z(z_mu)\n",
    "\n",
    "decoder = tf.make_template('decoder', P)\n",
    "X_samples_rand = decoder(z, bsize)\n",
    "X_samples = decoder(z_sample, bsize)\n",
    "\n",
    "# reconstruction loss\n",
    "recon_loss = tf.keras.losses.MSE(X, X_samples)#tf.losses.mean_squared_error(X, logits)\n",
    "#tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=X), 1)\n",
    "\n",
    "# D_KL(Q(z|X) || P(z|X)); calculate in closed form as both dist. are Gaussian\n",
    "#kl_loss = tf.constant([0.0]) #0.5 * tf.reduce_mean(tf.exp(z_logvar)/(prior_std**2) + z_mu**2/(prior_std**2) - 1. - z_logvar + np.log(prior_std**2), 1) #tf.constant(0.0) #0.5 * tf.reduce_sum(tf.exp(z_logvar) + z_mu**2 - 1. - z_logvar, 1)\n",
    "\n",
    "# VAE loss\n",
    "vae_loss = tf.reduce_mean(recon_loss)\n",
    "\n",
    "solver = tf.train.AdamOptimizer().minimize(vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(max_to_keep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "valid_losses = []\n",
    "\n",
    "training_AE_R2 = []\n",
    "validation_AE_R2 = []\n",
    "test_AE_R2 = []\n",
    "\n",
    "n_epochs = 10000 # number of epochs\n",
    "best_loss = 1e8\n",
    "stopping_step = 0\n",
    "patience = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # training\n",
    "    random_perm = np.random.permutation(X_data.shape[0])\n",
    "    mini_batch_index = 0\n",
    "    while True:\n",
    "        indices = random_perm[mini_batch_index:mini_batch_index+mb_size]\n",
    "        sess.run(solver, feed_dict={X: X_data[indices], bsize: X_data[indices].shape[0]})\n",
    "        mini_batch_index += mb_size\n",
    "        if mini_batch_index >= X_data.shape[0]:\n",
    "            break\n",
    "\n",
    "    # metrics       \n",
    "    clear_output(wait=True)\n",
    "\n",
    "    loss, rec = sess.run([vae_loss, recon_loss], feed_dict={X: X_data, bsize: split})\n",
    "    valid_loss, valid_rec = sess.run([vae_loss, recon_loss], feed_dict={X: X_data_valid, bsize: test_valid})\n",
    "\n",
    "    losses.append(loss)\n",
    "    #losses_rec.append(np.mean(rec))\n",
    "\n",
    "    valid_losses.append(valid_loss)\n",
    "    #valid_losses_rec.append(np.mean(valid_rec))\n",
    "\n",
    "\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    print('Loss: {:.4}'.format(np.mean(rec)))\n",
    "    print('Validation loss: {:.4}'.format(np.mean(valid_rec)))\n",
    "\n",
    "    #X_R2, _, _ = next_batch(X_data, X_data, 2000)\n",
    "    #prediction_training = sess.run(X_samples, feed_dict={X: X_R2, bsize: split})\n",
    "    #calculate_R2(X_R2, prediction_training, 'Training AE: ', training_AE_R2)\n",
    "    #prediction_validation = sess.run(X_samples, feed_dict={X: X_data_valid, bsize: test_valid})\n",
    "    #calculate_R2(X_data_valid, prediction_validation, 'Validation AE: ', validation_AE_R2)\n",
    "    #prediction_test = sess.run(X_samples, feed_dict={X: X_data_test, bsize: test_valid})\n",
    "    #calculate_R2(X_data_test, prediction_test, 'Test AE: ', test_AE_R2)\n",
    "\n",
    "    loss_value = np.mean(valid_rec)\n",
    "    if loss_value < best_loss:\n",
    "        stopping_step = 0\n",
    "        best_loss = loss_value\n",
    "        save_path = saver.save(sess, f\"./saved_models_iso_AEplusGP_AE/best_model.ckpt\")\n",
    "    else:\n",
    "        stopping_step += 1\n",
    "        print(f'Early stopping: {stopping_step}/{patience}')\n",
    "    if stopping_step >= patience:\n",
    "        print(f'Patience limit reached at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses, label='Training')        \n",
    "plt.plot(valid_losses, label='Validation')        \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best AE model\n",
    "#nprint(f'Best model was model {len(losses) - patience}')\n",
    "load_path = (\"./saved_models_iso_AEplusGP_AE/best_model.ckpt\")\n",
    "saver.restore(sess, load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values_train = sess.run(z_sample, feed_dict={X: X_data, bsize: split})\n",
    "z_values_valid = sess.run(z_sample, feed_dict={X: X_data_valid, bsize: test_valid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zmean = np.mean(z_values_train)\n",
    "zstd = np.std(z_values_train)\n",
    "z_values_train = (z_values_train - zmean)/zstd\n",
    "z_values_valid = (z_values_valid - zmean)/zstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_GP = np.zeros((test_valid, z_dim))\n",
    "# fit a GP to the components\n",
    "for component in range(z_dim):\n",
    "    ker = GPy.kern.Matern32(4,ARD=True)\n",
    "    m = GPy.models.GPRegression(y_data, z_values_train[:, component].reshape(-1, 1), ker)\n",
    "    m.optimize(messages=True,max_f_eval = 1000)\n",
    "\n",
    "    y_pred_train = m.predict(y_data)[0]\n",
    "    y_pred_train = y_pred_train[:, 0]\n",
    "    calculate_R2( z_values_train[:, component], y_pred_train, ' component R2 training: ', [], component)\n",
    "\n",
    "    y_pred_valid = m.predict(y_data_valid)[0]\n",
    "    y_pred_valid = y_pred_valid[:, 0]\n",
    "    calculate_R2(z_values_valid[:, component], y_pred_valid, ' component R2 validation: ', [], component)\n",
    "\n",
    "    y_pred_test = m.predict(y_data_test)[0]\n",
    "    fitted_GP[:, component] = y_pred_test[:, 0]\n",
    "\n",
    "    np.save(f'./saved_models_iso_AEplusGP/GPmodel_{component}.npy', m.param_array)\n",
    "    #plt.plot(shift_index_test, color='blue')\n",
    "    #plt.show()\n",
    "    #plt.plot(y_pred_test_2, color='red')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit GP to data rescaling\n",
    "# amplitude\n",
    "kern = GPy.kern.Matern32(4,ARD=True)\n",
    "n = GPy.models.GPRegression(y_data, amplitude_rescale_train, kern)\n",
    "n.optimize(messages=True, max_f_eval = 1000)\n",
    "\n",
    "y_pred_train = n.predict(y_data)[0]\n",
    "y_pred_train = y_pred_train[:, 0]\n",
    "calculate_R2(amplitude_rescale_train.flatten(), y_pred_train, 'Amplitude R2 train: ', [])\n",
    "\n",
    "y_pred_valid = n.predict(y_data_valid)[0]\n",
    "y_pred_valid = y_pred_valid[:, 0]\n",
    "calculate_R2(amplitude_rescale_valid, y_pred_valid, 'Amplitude R2 validation: ', [])\n",
    "\n",
    "# this is to be used later\n",
    "y_pred_test = n.predict(y_data_test)[0]\n",
    "y_pred_test = y_pred_test[:, 0]\n",
    "\n",
    "#plt.plot(amplitude_rescale_test, color='blue')\n",
    "#plt.show()\n",
    "#plt.plot(y_pred_test, color='red')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP training for Amplitude and Time shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_rescale = np.loadtxt('./amplitude_rescale_NOTsorted.txt')\n",
    "shift_index = np.loadtxt('./shift_index_NOTsorted.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_rescale_train = amplitude_rescale[:split].reshape(X_data.shape[0], 1)\n",
    "amplitude_rescale_valid = amplitude_rescale[split:split+test_valid]\n",
    "amplitude_rescale_test = amplitude_rescale[split+test_valid:]\n",
    "\n",
    "shift_index_train = shift_index[:split].reshape(X_data.shape[0], 1)\n",
    "shift_index_valid = shift_index[split:split+test_valid]\n",
    "shift_index_test = shift_index[split+test_valid:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time shift\n",
    "ker = GPy.kern.Matern32(4,ARD=True)\n",
    "m = GPy.models.GPRegression(y_data,shift_index_train,ker)\n",
    "m.optimize(messages=True,max_f_eval = 1000)\n",
    "\n",
    "y_pred_train_2 = m.predict(y_data)[0]\n",
    "y_pred_train_2 = y_pred_train_2[:, 0]\n",
    "calculate_R2(shift_index_train.flatten(), y_pred_train_2, 'Time shift R2 training: ', [])\n",
    "\n",
    "y_pred_valid_2 = m.predict(y_data_valid)[0]\n",
    "y_pred_valid_2 = y_pred_valid_2[:, 0]\n",
    "calculate_R2(shift_index_valid, y_pred_valid_2, 'Time shift R2 validation: ', [])\n",
    "\n",
    "y_pred_test_2 = m.predict(y_data_test)[0]\n",
    "y_pred_test_2 = y_pred_test_2[:, 0]\n",
    "\n",
    "#plt.plot(shift_index_test, color='blue')\n",
    "#plt.show()\n",
    "#plt.plot(y_pred_test_2, color='red')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_z_test = fitted_GP * zstd + zmean\n",
    "prediction_testing = sess.run(X_samples_rand, feed_dict={z: prediction_z_test, bsize: test_valid})\n",
    "    \n",
    "prediction_testing = np.multiply(prediction_testing, np.repeat(1/y_pred_test, X_dim).reshape(-1, X_dim))\n",
    "for index_seism in range(test_valid):\n",
    "    prediction_testing[index_seism] = shift(prediction_testing[index_seism], -y_pred_test_2[index_seism], cval=0.)\n",
    "\n",
    "# retrieve the unprocessed data back\n",
    "X_data_preprocessed = preprocess_seismo(X_data_, split, log=False, std=False, rescale=False, rescale_onlyamp=False)\n",
    "X_data_test = X_data_preprocessed[split+test_valid:]\n",
    "\n",
    "calculate_R2(X_data_test, prediction_testing, 'Final R2 testing: ', [])\n",
    "\n",
    "#for i in range(5):\n",
    "#    plt.plot(X_data_test[i], color='blue')\n",
    "#    plt.plot(prediction_testing[i], color='red')\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_GP = []\n",
    "# fit a GP to the components\n",
    "for component in range(z_dim):\n",
    "    ker = GPy.kern.Matern32(4,ARD=True)\n",
    "    m = GPy.models.GPRegression(y_data, z_values_train[:, component].reshape(-1, 1), ker)\n",
    "    m.update_model(False) # do not call the underlying expensive algebra on load\n",
    "    m.initialize_parameter() # Initialize the parameters (connect the parameters up)\n",
    "    m[:] = np.load('./saved_models_iso_AEplusGP/GPmodel_{}.npy'.format(component)) # Load the parameters\n",
    "    m.update_model(True) # do not call the underlying expensive algebra on load\n",
    "    fitted_GP.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ker = GPy.kern.Matern32(4,ARD=True) \n",
    "m_load = GPy.models.GPRegression(y_data, amplitude_rescale_train, ker, initialize=False)\n",
    "m_load.update_model(False) # do not call the underlying expensive algebra on load\n",
    "m_load.initialize_parameter() # Initialize the parameters (connect the parameters up)\n",
    "m_load[:] = np.load('./saved_models_iso_AEplusGP/GPmodel_amplitude.npy') # Load the parameters\n",
    "m_load.update_model(True) # Call the algebra only once\n",
    "\n",
    "kern = GPy.kern.Matern32(4,ARD=True) \n",
    "n_load = GPy.models.GPRegression(y_data, shift_index_train, kern, initialize=False)\n",
    "n_load.update_model(False) # do not call the underlying expensive algebra on load\n",
    "n_load.initialize_parameter() # Initialize the parameters (connect the parameters up)\n",
    "n_load[:] = np.load('./saved_models_iso_AEplusGP/GPmodel_time.npy') # Load the parameters\n",
    "n_load.update_model(True) # Call the algebra only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_inference = time.time()\n",
    "\n",
    "coordinate = np.array([31,25,158])\n",
    "\n",
    "shifted = coordinate - np.array([41,41,244])\n",
    "distances = np.linalg.norm(shifted)\n",
    "new_coords = np.zeros(4)\n",
    "new_coords[:3] = shifted\n",
    "new_coords[-1] = distances\n",
    "new_coords = (new_coords - meancoords)/stdcoords\n",
    "new_coords = new_coords.reshape((1,y_dim))\n",
    "\n",
    "predAE = np.zeros(z_dim)\n",
    "for i in range(z_dim):\n",
    "    predAE_tmp = fitted_GP[i].predict(new_coords)[0]\n",
    "    predAE[i] = predAE_tmp[:, 0]\n",
    "prediction_z_test = predAE * zstd + zmean\n",
    "prediction_z_test = prediction_z_test.reshape(1, z_dim)\n",
    "prediction_testing = sess.run(X_samples_rand, feed_dict={z: prediction_z_test})\n",
    "\n",
    "y_pred_test = m_load.predict(new_coords)[0]\n",
    "y_pred_test = y_pred_test[:, 0]\n",
    "y_pred_test_2 = n_load.predict(new_coords)[0]\n",
    "y_pred_test_2 = y_pred_test_2[:, 0]\n",
    "prediction_testing = np.multiply(prediction_testing, np.repeat(1/y_pred_test, X_dim).reshape(-1, X_dim))\n",
    "prediction_testing = shift(prediction_testing[0], -y_pred_test_2, cval=0.)\n",
    "\n",
    "timeinf = time.time() - start_time_inference\n",
    "print(\"timeinf\", timeinf)\n",
    "np.save(\"AE_plus_GP_inftime.npy\", timeinf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
